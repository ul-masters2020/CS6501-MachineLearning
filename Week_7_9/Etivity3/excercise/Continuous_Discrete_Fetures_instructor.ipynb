{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('winequality.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Winedataset\n",
    "The problem of predicting quality given the wine features is a regression problem:\n",
    "* because the output (i.e., quality) is continuous \n",
    "Quality is an ordinal, that is\n",
    "\n",
    "$$\n",
    "0 \\leq 1 \\leq 2 \\leq \\dots \\leq 6\n",
    "$$\n",
    "\n",
    "a 5-quality wine is \n",
    "* better than a 4-quality wine and \n",
    "* worse than a 6-quality wine.\n",
    "\n",
    "That's the reason we formulated the problem as a regression problem, we want to predict numerical values. \n",
    "Linear regression could predict a quality that is 5.1 (even if fractional qualities are not in the dataset) and it means that wine is slightly better than a 5-quality wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now imagine we want  to consider quality a **categorical variable**, that is\n",
    "\n",
    "$$\n",
    "0,1,2,3,4,5,6\n",
    "$$\n",
    "\n",
    "are classes. This problem then becomes a classification problem\n",
    "* because the output (i.e., quality) is discrete\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Can we use MultinomialNB to predict the quality-class given the wine-characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No because wine-characteristics are continuous variables abd MultinomialNB only accepts counts.\n",
    "\n",
    "For instance, in the gender-name example, we had to use this encoding\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a &\\rightarrow & [1,0,\\dots,0]\\\\\n",
    "b &\\rightarrow & [0,1,\\dots,0]\\\\\n",
    "... & ... & ....\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "so the dataset $\\{(a,1),(b,1)\\}$ is encoded into\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1,0,0,\\dots,0,1\\\\\n",
    "0,1,0,\\dots,0,1\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the last column is the class variable (gender)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discretisation and encoding\n",
    "\n",
    "We can discretise a continuous variables by using bins (interval). Consider for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.4 7.8 7.8 ... 6.3 5.9 6. ]\n",
      "4.6 15.9\n"
     ]
    }
   ],
   "source": [
    "fixed_ac = dataset['fixed acidity'].values\n",
    "print(fixed_ac)\n",
    "print(np.min(fixed_ac),np.max(fixed_ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the interval $[4.6, 15.9]$ in five equally spaced intervals, e.g.,\n",
    "\n",
    "$$\n",
    "[ 4.6 ,  6.86], ~~[6.86,  9.12], ~~[9.12, 11.38], ~~[11.38,13.64], ~~[13.64, 15.9 ]\n",
    "$$\n",
    "\n",
    "and we can consider these five intervals as five classes.\n",
    "This is what we do for instance when we plot histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([274., 913., 298., 102.,  12.]),\n",
       " array([ 4.6 ,  6.86,  9.12, 11.38, 13.64, 15.9 ]),\n",
       " <BarContainer object of 5 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Biining a continuous variable\n",
    "plt.hist(fixed_ac,bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a faster way to that using a function called `KBinsDiscretizer` from `sklearn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.4],\n",
       "       [7.8],\n",
       "       [7.8],\n",
       "       ...,\n",
       "       [6.3],\n",
       "       [5.9],\n",
       "       [6. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_ac.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = fixed_ac.reshape(-1,1)\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "est = KBinsDiscretizer(n_bins=5,encode='ordinal',strategy='uniform')\n",
    "est.fit(X)\n",
    "X_d = est.transform(X)\n",
    "X_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99],\n",
       "       [7.99],\n",
       "       [7.99],\n",
       "       ...,\n",
       "       [5.73],\n",
       "       [5.73],\n",
       "       [5.73]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.inverse_transform(X_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.73]]\n",
      "[[7.99]]\n",
      "[[10.25]]\n",
      "[[12.51]]\n",
      "[[14.77]]\n"
     ]
    }
   ],
   "source": [
    "print(est.inverse_transform(np.array([[0]])))\n",
    "print(est.inverse_transform(np.array([[1]])))\n",
    "print(est.inverse_transform(np.array([[2]])))\n",
    "print(est.inverse_transform(np.array([[3]])))\n",
    "print(est.inverse_transform(np.array([[4]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to apply MultinomialNB we need\n",
    "1. to discretize the variables in 5 categories (0,1,2,3,4)\n",
    "2. to encode the 5 categories\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 &\\rightarrow & [1,0,0,0,0]\\\\\n",
    "1 &\\rightarrow & [0,1,0,0,0]\\\\\n",
    "2 &\\rightarrow & [0,0,1,0,0]\\\\\n",
    "3 &\\rightarrow & [0,0,0,1,0]\\\\\n",
    "4 &\\rightarrow & [0,0,0,0,1]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can do it in one-shot by using `KBinsDiscretizer(n_bins=5,encode='onehot',strategy='uniform')`\n",
    "In this way, the inputs are already in the right format for MultinomialNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1599x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1599 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = fixed_ac.reshape(-1,1)\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "est = KBinsDiscretizer(n_bins=5,encode='onehot',strategy='uniform')\n",
    "est.fit(X)\n",
    "X_d = est.transform(X)\n",
    "X_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`Xd_train` is saved as a sparse-matrix so we do not need to save all the zeros and in this way we save memory\n",
    "space. If we want to see it, we can transform it back to a normal (dense) matrix with the following instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can transforma sparse matrix into a normal matrix as:\n",
    "X_d.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step1: we divide the dataset in training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(959, 11)\n",
      "(640, 11)\n"
     ]
    }
   ],
   "source": [
    "#we split the dataset in training and testing\n",
    "X=dataset.iloc[:,0:-1].values\n",
    "y=dataset.iloc[:,-1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step2: we discretize the inputs in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<959x55 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10549 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = KBinsDiscretizer(n_bins=5,encode='onehot',strategy='uniform')\n",
    "est.fit(X_train)\n",
    "Xd_train = est.transform(X_train)\n",
    "Xd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_test[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step3: we discretize the inputs in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_test  = est.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we  **don't do**\n",
    "\n",
    "`est = KBinsDiscretizer(n_bins=10,encode='onehot',strategy='quantile')\n",
    "est.fit(X_test)\n",
    "Xd_test = est.transform(X_test)\n",
    "Xd_train`\n",
    "\n",
    "A discretisation algorithm is a ML algorithm and, therefore, we do `fit` only on the training dataset and we only transform the test dataset. Otherwise, we could risk overfitting.\n",
    "\n",
    "This is a **general approach**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General ML Recipe algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB() # \n",
    "\n",
    "#training\n",
    "clf.fit(Xd_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(Xd_train)\n",
    "y_test_pred  = clf.predict(Xd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training set  0.5901981230448383\n",
      "accuracy test set  0.5640625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(\"accuracy training set \", accuracy_score(y_train,y_train_pred))\n",
    "print(\"accuracy test set \",accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix test set \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   1,   0,   0,   0],\n",
       "       [  0,   2,  11,  10,   0,   0],\n",
       "       [  0,   1, 190,  78,   7,   2],\n",
       "       [  0,   1,  88, 126,  32,   1],\n",
       "       [  0,   0,   5,  32,  43,   1],\n",
       "       [  0,   0,   0,   4,   4,   0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion Matrix test set \")\n",
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question \n",
    "How do we select the number of bins in `KBinsDiscretizer(n_bins=7,encode='onehot',strategy='quantile')`?\n",
    "\n",
    "Is there an optimal way (the discretisation that gives us the highest accuracy on the test set)?\n",
    "\n",
    "To answer this question we can use 10-folds cross-validation. This will help us\n",
    "to avoid overfitting. We select the value n_bins that provides the highest accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'CV.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1297\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'CV.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'CV.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1297\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'CV.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"CV.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop computes the 10-fold cross-validation accuracy for a fixed value of `n_bins`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross-validation accuracy= 0.5834866352201258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "n_bins = 7\n",
    "k_fold = KFold(n_splits=10,  shuffle=True, random_state=42)\n",
    "Accuracy=[]\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "    #discretise\n",
    "    est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "    est.fit(X_train)\n",
    "    Xd_train = est.transform(X_train)\n",
    "    Xd_test  = est.transform(X_test)\n",
    "    clf = MultinomialNB() \n",
    "    #training\n",
    "    clf.fit(Xd_train, y_train)\n",
    "    #prediction\n",
    "    y_test_pred  = clf.predict(Xd_test)\n",
    "    #accuracy\n",
    "    Accuracy.append(accuracy_score(y_test,y_test_pred))\n",
    "    \n",
    "print(\"Average 10-fold cross-validation accuracy=\",np.mean(np.array(Accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Implement a for-loop that tries all the values of `n_bins` in the  `range(2,20)`  and\n",
    "selects the value `n_bins` corresponding to the highest  best **Average 10-fold cross-validation accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.          0.51090016]\n",
      " [ 3.          0.53285377]\n",
      " [ 4.          0.55407233]\n",
      " [ 5.          0.55155267]\n",
      " [ 6.          0.54847091]\n",
      " [ 7.          0.56910377]\n",
      " [ 8.          0.55595519]\n",
      " [ 9.          0.56159984]\n",
      " [10.          0.55720519]\n",
      " [11.          0.55660377]\n",
      " [12.          0.55720519]\n",
      " [13.          0.56722484]\n",
      " [14.          0.56784591]\n",
      " [15.          0.55156447]\n",
      " [16.          0.56533805]\n",
      " [17.          0.54782626]\n",
      " [18.          0.55032626]\n",
      " [19.          0.5559316 ]]\n",
      "Best n_bins= [7.         0.56910377]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=10)\n",
    "Accuracy_Fold=[]\n",
    "for n_bins in range(2,20):\n",
    "    Accuracy=[]\n",
    "    for train_indices, test_indices in k_fold.split(X):\n",
    "        #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "        X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "        #discretise\n",
    "        est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "        est.fit(X_train)\n",
    "        Xd_train = est.transform(X_train)\n",
    "        Xd_test  = est.transform(X_test)\n",
    "        clf = MultinomialNB() \n",
    "        #training\n",
    "        clf.fit(Xd_train, y_train)\n",
    "\n",
    "        #prediction\n",
    "        y_test_pred  = clf.predict(Xd_test)\n",
    "        #accuracy\n",
    "        Accuracy.append(accuracy_score(y_test,y_test_pred))\n",
    "    #print(\"Average 10-fold cross-validation accuracy=\",np.mean(np.array(Accuracy)))\n",
    "    Accuracy_Fold.append([n_bins,np.mean(np.array(Accuracy))])\n",
    "Accuracy_Fold=np.array(Accuracy_Fold)\n",
    "print(Accuracy_Fold)\n",
    "ind=np.argmax(Accuracy_Fold[:,1])\n",
    "print(\"Best n_bins=\",Accuracy_Fold[ind,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression classifier\n",
    "\n",
    "Can we use a classifier than doesn't need discretisation?\n",
    "Equivalently, can we use directly continuous inputs without discretisation?\n",
    "\n",
    "\n",
    "Logistic Regression classifier is the simplest neural network. It is a classifier that directly accepts\n",
    "continuous inputs. We will go into details during the lecture on Monday; we will use it now as a blackbox model,\n",
    "that is without going into details of the algorithm.\n",
    "\n",
    "Note that, although is called **Logistic Regression** it is actually  a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benavoli/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#we split the dataset in training and testing\n",
    "X=dataset.iloc[:,0:-1].values\n",
    "y=dataset.iloc[:,-1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "#Logistic\n",
    "clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial')#'multinomial' here means that the class\n",
    "                                                                   #has more than two categories\n",
    "clf.fit(X_train,y_train)\n",
    "y_train_pred2= clf.predict(X_train)\n",
    "y_test_pred2 = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training set  0.5922836287799792\n",
      "accuracy test set  0.5546875\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy training set \", accuracy_score(y_train_pred2,y_train))\n",
    "print(\"accuracy test set \",accuracy_score(y_test_pred2,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving the convergence problem by normalsing the inputs\n",
    "When you run LogisticRegression you have this warning\n",
    "\n",
    "`...python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
    "  \"of iterations.\", ConvergenceWarning)\n",
    "`\n",
    "\n",
    "This is a numerical problem, the optimiser lbfgs, that numerically maximises the likelihood to find the MLE, is not converging (more details on Monday). \n",
    "\n",
    "This is a common problem in ML. A way to help lbfgs is by scaling the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler()\n",
    "scalerX.fit(X_train)\n",
    "X_tr_scaled = scalerX.transform(X_train)\n",
    "X_te_scaled = scalerX.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard scaler normalize the data, that is for each column in X_train does the following\n",
    "\n",
    "`\n",
    "(X_train[:,i]-np.mean(X_train[:,i]))/np.std(X_train[:,i])\n",
    "`\n",
    "\n",
    "where `mean` computes the mean and `std` the standard deviation of the column.\n",
    "\n",
    "Example: assume the data is $[10,50,35,20]$, then \n",
    "\n",
    "$$\n",
    "mean=\\frac{10+50+35+20}{4}=28.75\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "std=\\sqrt{\\frac{(10-28.75)^2+(50-28.75)^2+(35-28.75)^2+(20-28.75)^2}{4}}=1.5155\n",
    "$$\n",
    "\n",
    "and so the transformed data is\n",
    "\n",
    "$$\n",
    "\\frac{[10,50,35,20]-28.75}{15.155}=[-1.23717915,  1.40213637,  0.41239305, -0.57735027]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "col=0\n",
    "#before\n",
    "plt.hist(X_train[:,col]);\n",
    "plt.figure()\n",
    "#after\n",
    "plt.hist(X_tr_scaled[:,col]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training set  0.6308654848800834\n",
      "accuracy test set  0.56875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial')#'multinomial' here means that the class\n",
    "                                                                   #has more than two categories\n",
    "clf.fit(X_tr_scaled,y_train)\n",
    "y_train_pred2= clf.predict(X_tr_scaled)\n",
    "y_test_pred2 = clf.predict(X_te_scaled)\n",
    "print(\"accuracy training set \", accuracy_score(y_train_pred2,y_train))\n",
    "print(\"accuracy test set \",accuracy_score(y_test_pred2,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now  the classifier has  a better perfomance, because lbfgs computed a better solution (We will go into details on Monday).\n",
    "\n",
    "Is scaling+LogisticRegression better than discretisation + MultinomialNB?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 2\n",
    "Use 10-fold cross validation to compare the performance of the two classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross-validation accuracy MultinomialNB= 0.5691037735849058\n",
      "Average 10-fold cross-validation accuracy Logistic= 0.5891155660377358\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=10)\n",
    "n_bins = 7\n",
    "\n",
    "Accuracy_MNB=[]\n",
    "Accuracy_Logistic=[]\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "    #discretise\n",
    "    est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "    est.fit(X_train)\n",
    "    Xd_train = est.transform(X_train)\n",
    "    Xd_test  = est.transform(X_test)\n",
    "    clf = MultinomialNB() \n",
    "    #training\n",
    "    clf.fit(Xd_train, y_train)\n",
    "    #prediction\n",
    "    y_test_pred  = clf.predict(Xd_test)\n",
    "    #accuracy\n",
    "    Accuracy_MNB.append(accuracy_score(y_test,y_test_pred))\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(X_train)\n",
    "    X_tr_scaled = scalerX.transform(X_train)\n",
    "    X_te_scaled = scalerX.transform(X_test)\n",
    "    \n",
    "    clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial',max_iter=3000)#'multinomial' here means that the class\n",
    "                                                                    #has more than two categories ,max_iter=2000\n",
    "    clf.fit(X_tr_scaled,y_train)\n",
    "\n",
    "    y_test_pred_logistic = clf.predict(X_te_scaled)\n",
    "    Accuracy_Logistic.append(accuracy_score(y_test,y_test_pred_logistic))\n",
    "    \n",
    "print(\"Average 10-fold cross-validation accuracy MultinomialNB=\",np.mean(np.array(Accuracy_MNB)))\n",
    "print(\"Average 10-fold cross-validation accuracy Logistic=\",np.mean(np.array(Accuracy_Logistic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the regularisation parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross-validation accuracy MultinomialNB= 0.5778577044025157\n",
      "Average 10-fold cross-validation accuracy Logistic= 0.5897405660377358\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=10)\n",
    "n_bins = 7\n",
    "\n",
    "Accuracy_MNB=[]\n",
    "Accuracy_Logistic=[]\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "    #discretise\n",
    "    est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "    est.fit(X_train)\n",
    "    Xd_train = est.transform(X_train)\n",
    "    Xd_test  = est.transform(X_test)\n",
    "    clf = MultinomialNB(alpha=10) \n",
    "    #training\n",
    "    clf.fit(Xd_train, y_train)\n",
    "    #prediction\n",
    "    y_test_pred  = clf.predict(Xd_test)\n",
    "    #accuracy\n",
    "    Accuracy_MNB.append(accuracy_score(y_test,y_test_pred))\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(X_train)\n",
    "    X_tr_scaled = scalerX.transform(X_train)\n",
    "    X_te_scaled = scalerX.transform(X_test)\n",
    "    \n",
    "    clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial',max_iter=2000, C=2)#'multinomial' here means that the class\n",
    "                                                                    #has more than two categories ,max_iter=2000\n",
    "    clf.fit(X_tr_scaled,y_train)\n",
    "\n",
    "    y_test_pred_logistic = clf.predict(X_te_scaled)\n",
    "    Accuracy_Logistic.append(accuracy_score(y_test,y_test_pred_logistic))\n",
    "    \n",
    "print(\"Average 10-fold cross-validation accuracy MultinomialNB=\",np.mean(np.array(Accuracy_MNB)))\n",
    "print(\"Average 10-fold cross-validation accuracy Logistic=\",np.mean(np.array(Accuracy_Logistic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Business Problem: Banks receives so many applications for credit card (CC) request. Going through each request manually can be very time consuming, also prone to human errors. However, if we can use the historical data to build a model which can shortlist the candidates for approval that save time.\n",
    "\n",
    "By using the previous two classifiers, you must build a predictor model that predicts if a CC should be approved\n",
    "for a person that has certain characteristics (sex,age,Debt, Married etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Male</th>\n",
       "      <th>Age</th>\n",
       "      <th>Debt</th>\n",
       "      <th>Married</th>\n",
       "      <th>BankCustomer</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>YearsEmployed</th>\n",
       "      <th>PriorDefault</th>\n",
       "      <th>Employed</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Citizen</th>\n",
       "      <th>Income</th>\n",
       "      <th>Approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1</td>\n",
       "      <td>21.08</td>\n",
       "      <td>10.085</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0</td>\n",
       "      <td>22.67</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0</td>\n",
       "      <td>25.25</td>\n",
       "      <td>13.500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>1</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Male    Age    Debt  Married  BankCustomer  EducationLevel  Ethnicity  \\\n",
       "0       1  30.83   0.000        1             0              12          7   \n",
       "1       0  58.67   4.460        1             0              10          3   \n",
       "2       0  24.50   0.500        1             0              10          3   \n",
       "3       1  27.83   1.540        1             0              12          7   \n",
       "4       1  20.17   5.625        1             0              12          7   \n",
       "..    ...    ...     ...      ...           ...             ...        ...   \n",
       "685     1  21.08  10.085        2             2               4          3   \n",
       "686     0  22.67   0.750        1             0               1          7   \n",
       "687     0  25.25  13.500        2             2               5          2   \n",
       "688     1  17.92   0.205        1             0               0          7   \n",
       "689     1  35.00   3.375        1             0               1          3   \n",
       "\n",
       "     YearsEmployed  PriorDefault  Employed  CreditScore  Citizen  Income  \\\n",
       "0             1.25             1         1            1        0       0   \n",
       "1             3.04             1         1            6        0     560   \n",
       "2             1.50             1         0            0        0     824   \n",
       "3             3.75             1         1            5        0       3   \n",
       "4             1.71             1         0            0        2       0   \n",
       "..             ...           ...       ...          ...      ...     ...   \n",
       "685           1.25             0         0            0        0       0   \n",
       "686           2.00             0         1            2        0     394   \n",
       "687           2.00             0         1            1        0       1   \n",
       "688           0.04             0         0            0        0     750   \n",
       "689           8.29             0         0            0        0       0   \n",
       "\n",
       "     Approved  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "..        ...  \n",
       "685         1  \n",
       "686         1  \n",
       "687         1  \n",
       "688         1  \n",
       "689         1  \n",
       "\n",
       "[690 rows x 14 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"CreditCard_cleanedData.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new variable to input features and labels\n",
    "X,y = df.iloc[:,0:13].values , df.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3 (same solution has before, but one should first select the optimal number of bins\n",
    "as in Exercise 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross-validation accuracy MultinomialNB= 0.8391304347826087\n",
      "Average 10-fold cross-validation accuracy Logistic= 0.8405797101449277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=10)\n",
    "n_bins = 7\n",
    "\n",
    "Accuracy_MNB=[]\n",
    "Accuracy_Logistic=[]\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    #print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "    X_train, X_test, y_train, y_test =X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "    #discretise\n",
    "    est = KBinsDiscretizer(n_bins=n_bins,encode='onehot',strategy='uniform')\n",
    "    est.fit(X_train)\n",
    "    Xd_train = est.transform(X_train)\n",
    "    Xd_test  = est.transform(X_test)\n",
    "    clf = MultinomialNB() \n",
    "    #training\n",
    "    clf.fit(Xd_train, y_train)\n",
    "    #prediction\n",
    "    y_test_pred  = clf.predict(Xd_test)\n",
    "    #accuracy\n",
    "    Accuracy_MNB.append(accuracy_score(y_test,y_test_pred))\n",
    "    \n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(X_train)\n",
    "    X_tr_scaled = scalerX.transform(X_train)\n",
    "    X_te_scaled = scalerX.transform(X_test)\n",
    "    \n",
    "    clf  = LogisticRegression(solver='lbfgs', multi_class='multinomial')#'multinomial' here means that the class\n",
    "                                                                    #has more than two categories\n",
    "    clf.fit(X_tr_scaled,y_train)\n",
    "\n",
    "    y_test_pred_logistic = clf.predict(X_te_scaled)\n",
    "    Accuracy_Logistic.append(accuracy_score(y_test,y_test_pred_logistic))\n",
    "    \n",
    "print(\"Average 10-fold cross-validation accuracy MultinomialNB=\",np.mean(np.array(Accuracy_MNB)))\n",
    "print(\"Average 10-fold cross-validation accuracy Logistic=\",np.mean(np.array(Accuracy_Logistic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
