{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Spam Classification\n",
    "Deciding whether an email is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "df=pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df=df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis='columns')\n",
    "\n",
    "#df[v1] is the class variable and df[v2] is the  email\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: removing stopwords and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer('english')\n",
    "#A  stemming algorithm reduces words like fishing, fished, and fisher to the stem fish.\n",
    "#The stem need not be a word, for example  argue, argued, \n",
    "#argues, arguing, and argus could be reduced to the stem argu. \n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "#Stop words are  the most common words in a language\n",
    "#and are filtered out before processing of natural language data \n",
    "\n",
    "\n",
    "df['v2']=[re.sub('[^a-zA-Z]', ' ', sms) for sms in df['v2']]\n",
    "word_list=[sms.split() for sms in df['v2']]\n",
    "def normalize(words):\n",
    "    current_words=list()\n",
    "    for word in words:\n",
    "        if word.lower() not in stop: #remove  the most common words\n",
    "            updated_word=stemmer.stem(word) #stemming\n",
    "            current_words.append(updated_word.lower()) #lower case\n",
    "    return current_words\n",
    "word_list=[normalize(word) for word in word_list]\n",
    "df['v2']=[\" \".join(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri wkli comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>nd time tri contact u u pound prize claim easi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>piti mood suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>guy bitch act like interest buy someth els nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  go jurong point crazi avail bugi n great world...\n",
       "1      ham                              ok lar joke wif u oni\n",
       "2     spam  free entri wkli comp win fa cup final tkts st ...\n",
       "3      ham                u dun say earli hor u c alreadi say\n",
       "4      ham               nah think goe usf live around though\n",
       "...    ...                                                ...\n",
       "5567  spam  nd time tri contact u u pound prize claim easi...\n",
       "5568   ham                              b go esplanad fr home\n",
       "5569   ham                                  piti mood suggest\n",
       "5570   ham  guy bitch act like interest buy someth els nex...\n",
       "5571   ham                                     rofl true name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[v1] is the class variable and df[v2] is the processed email\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in training and testing\n",
    "x_train, x_test, y_train, y_test=train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: transforming email into numerical string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails= 4457\n",
      "number of words= 5595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<4457x5595 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it counts the words\n",
    "cv=CountVectorizer()\n",
    "#it returns the number of times a word appears in the i-th email\n",
    "x_train_df=cv.fit_transform(x_train) #x_train_df is a matrix emails times words\n",
    "print(\"number of emails=\",x_train_df.shape[0])\n",
    "print(\"number of words=\",x_train_df.shape[1])\n",
    "x_test_df=cv.transform(x_test)\n",
    "\n",
    "#this is a sparse matrix (it means that only non-zeroes elements are stored)\n",
    "x_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5595)\n",
      "this is the non-sparse matrix= [[0 0 0 ... 0 0 0]]\n",
      "\n",
      "wish great day moji told offer alway speechless offer easili go great length behalf stun exam next friday keep touch sorri\n",
      "\n",
      "[array(['alway', 'behalf', 'day', 'easili', 'exam', 'friday', 'go',\n",
      "       'great', 'keep', 'length', 'moji', 'next', 'offer', 'sorri',\n",
      "       'speechless', 'stun', 'told', 'touch', 'wish'], dtype='<U34')]\n",
      "\n",
      "[ 162  458 1140 1398 1552 1790 1921 1991 2541 2676 3047 3214 3328 4456\n",
      " 4483 4620 4944 4984 5405]\n",
      "\n",
      "[[1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "row_index=0 #select one email\n",
    "print(x_train_df[row_index,:].todense().shape)\n",
    "print(\"this is the non-sparse matrix=\",x_train_df[row_index,:].todense())\n",
    "ind=np.where(x_train_df[row_index,:].todense()[0,:]>0)[1]\n",
    "print()\n",
    "#original words in the email\n",
    "print(x_train.values[row_index])\n",
    "print()\n",
    "#decoded numerical input \n",
    "print(cv.inverse_transform(x_train_df[row_index,:].todense()))\n",
    "print()\n",
    "#index of those words in x_train_df[row_index,:].todense()\n",
    "print(ind)\n",
    "print()\n",
    "# number of times those words appears in the email\n",
    "print(x_train_df[row_index,ind].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: training the classifier and making predictions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train_df,y_train)\n",
    "prediction_train=clf.predict(x_train_df)\n",
    "prediction_test=clf.predict(x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: computing accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9923715503702042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#scores\n",
    "print(\"Accuracy:\"+str(accuracy_score(y_train,prediction_train)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We care about the generalisation error, that is the performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.989237668161435\n",
      "\n",
      "Confusion Matrix\n",
      "[[965   5]\n",
      " [  7 138]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#scores\n",
    "print(\"Accuracy:\"+str(accuracy_score(y_test,prediction_test)))\n",
    "print()\n",
    "\n",
    "conf_mat=confusion_matrix(y_test, prediction_test)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where can we find sparse matrices ?\n",
    "You can manipulate them using scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexes of non-zeroes elements= [ 162  458 1140 1398 1552 1790 1921 1991 2541 2676 3047 3214 3328 4456\n",
      " 4483 4620 4944 4984 5405]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sc #this is the library\n",
    "\n",
    "#x_train_df is a scipy sparse matrix, this avoids to store the zeroes\n",
    "#to access to the non-zero element\n",
    "i=0# email index\n",
    "ind=sc.find(x_train_df[i,:]>0)[1]\n",
    "print(\"indexes of non-zeroes elements=\",ind)\n",
    "x_train_df[0,ind].todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexes of non-zeroes elements= [2870 3588]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test set\n",
    "ind=sc.find(x_test_df[i,:]>0)[1]\n",
    "print(\"indexes of non-zeroes elements=\",ind)\n",
    "x_test_df[0,ind].todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider Movie Reviews Corpus, a dataset that includes  movie reviews that are categorized as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/aman/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "df = pd.DataFrame(columns=['v1', 'v2'])\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        df=df.append({'v1': category, 'v2': movie_reviews.words(fileid)}, ignore_index=True)\n",
    "        \n",
    "word_list=[sms for sms in df['v2']]\n",
    "def normalize(words):\n",
    "    current_words=list()\n",
    "    for word in words:\n",
    "        if word.lower() not in stop: #remove  the most common words\n",
    "            if word.isalpha(): #remove punctuation\n",
    "                updated_word=stemmer.stem(word) #stemming\n",
    "                current_words.append(updated_word.lower()) #lower case\n",
    "    return current_words\n",
    "word_list=[normalize(word) for word in word_list]\n",
    "df['v2']=[\" \".join(word) for word in word_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       plot two teen coupl go church parti drink driv...\n",
       "1       happi bastard quick movi review damn bug got h...\n",
       "2       movi like make jade movi viewer thank invent t...\n",
       "3       quest camelot warner bros first featur length ...\n",
       "4       synopsi mental unstabl man undergo psychothera...\n",
       "                              ...                        \n",
       "1995    wow movi everyth movi funni dramat interest we...\n",
       "1996    richard gere command actor alway great film ev...\n",
       "1997    glori star matthew broderick denzel washington...\n",
       "1998    steven spielberg second epic film world war ii...\n",
       "1999    truman true man burbank perfect name jim carre...\n",
       "Name: v2, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same steps as in the Spam filter example, apply MultinomialNB to this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in training and testing\n",
    "x_train, x_test, y_train, y_test=train_test_split(df['v2'], df['v1'], test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews= 1600\n",
      "number of words= 22746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1600x22746 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 398985 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step3\n",
    "#it counts the words\n",
    "cv=CountVectorizer()\n",
    "#it returns the number of times a word appears in the i-th email\n",
    "x_train_df=cv.fit_transform(x_train) #x_train_df is a matrix emails times words\n",
    "print(\"number of reviews=\",x_train_df.shape[0])\n",
    "print(\"number of words=\",x_train_df.shape[1])\n",
    "x_test_df=cv.transform(x_test)\n",
    "\n",
    "#this is a sparse matrix (it means that only non-zeroes elements are stored)\n",
    "x_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 22746)\n",
      "this is the non-sparse matrix= [[0 0 0 ... 0 0 0]]\n",
      "\n",
      "someon journey theater see comedi alway risk sit inan recent saw film like edtv offic space realli bad comedi hit miss moviego goe theater expect amus realli shame alleg comedi fail deliv weep dear reader latest comedi hollywood movi mill noth less sure bet austin power spi shag one funniest thing pleasur see long time complet looney delight parodi often pretenti jame bond flick comedi even half say even fifth could consist hyster would take resid local multiplex even movi credit fact sequel sleeper cult hit austin power intern man mysteri origin came nowher low budget eccentr movi mani expect flop like lesli nielsen parodi sudden quot teenag america iron power yeeeeah babi yeah almost becom icon late thus anoth instal inevit also undoubt welcom spi shag one unrestrain top comedi ever seen entir life bring back charact predecessor even briefli add new one austin power mike myer swing hipster transport go back time get back mojo oh figur dr evil meyer stolen enlist help gorgeous secret agent felic shagwel prior sidekick vanessa proper dispos hilari open sequenc play without much distinguish gusto heather graham togeth go back defeat dr evil yet surpris real star dr evil rather austin get screen time popular first movi perhap true portray written affection instead scheme villain mad scientist might expect charact like bumbl often sweet mad scientist wannab spi shag much stuff saw origin okay origin left us want time one near share shag joke crude pun power antic sequel satisfi appetit even conclus still sure fill bit like smoke sex know babi never look may seem aw immatur understand austin power seri unlik crap hollywood feed us day genuin funni know ever laugh harder movi jerri springer send indubit hilari two us rendit dr evil new miniatur clone name mini pretens particular smart social satir anyth sort inde littl beyond joy viewer feel see someth abl entertain much movi far concern enough eugen novikov\n",
      "\n",
      "[array(['abl', 'add', 'affection', 'agent', 'alleg', 'almost', 'also',\n",
      "       'alway', 'america', 'amus', 'anoth', 'antic', 'anyth', 'appetit',\n",
      "       'austin', 'aw', 'babi', 'back', 'bad', 'becom', 'bet', 'beyond',\n",
      "       'bit', 'bond', 'briefli', 'bring', 'budget', 'bumbl', 'came',\n",
      "       'charact', 'clone', 'comedi', 'complet', 'concern', 'conclus',\n",
      "       'consist', 'could', 'crap', 'credit', 'crude', 'cult', 'day',\n",
      "       'dear', 'defeat', 'delight', 'deliv', 'dispos', 'distinguish',\n",
      "       'dr', 'eccentr', 'edtv', 'enlist', 'enough', 'entertain', 'entir',\n",
      "       'eugen', 'even', 'ever', 'evil', 'expect', 'fact', 'fail', 'far',\n",
      "       'feed', 'feel', 'felic', 'fifth', 'figur', 'fill', 'film', 'first',\n",
      "       'flick', 'flop', 'funni', 'funniest', 'genuin', 'get', 'go', 'goe',\n",
      "       'gorgeous', 'graham', 'gusto', 'half', 'harder', 'heather', 'help',\n",
      "       'hilari', 'hipster', 'hit', 'hollywood', 'hyster', 'icon',\n",
      "       'immatur', 'inan', 'inde', 'indubit', 'inevit', 'instal',\n",
      "       'instead', 'intern', 'iron', 'jame', 'jerri', 'joke', 'journey',\n",
      "       'joy', 'know', 'late', 'latest', 'laugh', 'left', 'lesli', 'less',\n",
      "       'life', 'like', 'littl', 'local', 'long', 'look', 'looney', 'low',\n",
      "       'mad', 'man', 'mani', 'may', 'meyer', 'might', 'mike', 'mill',\n",
      "       'mini', 'miniatur', 'miss', 'mojo', 'movi', 'moviego', 'much',\n",
      "       'multiplex', 'myer', 'mysteri', 'name', 'near', 'never', 'new',\n",
      "       'nielsen', 'noth', 'novikov', 'nowher', 'offic', 'often', 'oh',\n",
      "       'okay', 'one', 'open', 'origin', 'parodi', 'particular', 'perhap',\n",
      "       'play', 'pleasur', 'popular', 'portray', 'power', 'predecessor',\n",
      "       'pretens', 'pretenti', 'prior', 'proper', 'pun', 'quot', 'rather',\n",
      "       'reader', 'real', 'realli', 'recent', 'rendit', 'resid', 'risk',\n",
      "       'satir', 'satisfi', 'saw', 'say', 'scheme', 'scientist', 'screen',\n",
      "       'secret', 'see', 'seem', 'seen', 'send', 'sequel', 'sequenc',\n",
      "       'seri', 'sex', 'shag', 'shagwel', 'shame', 'share', 'sidekick',\n",
      "       'sit', 'sleeper', 'smart', 'smoke', 'social', 'someon', 'someth',\n",
      "       'sort', 'space', 'spi', 'springer', 'star', 'still', 'stolen',\n",
      "       'stuff', 'sudden', 'sure', 'surpris', 'sweet', 'swing', 'take',\n",
      "       'teenag', 'theater', 'thing', 'thus', 'time', 'togeth', 'top',\n",
      "       'transport', 'true', 'two', 'understand', 'undoubt', 'unlik',\n",
      "       'unrestrain', 'us', 'vanessa', 'viewer', 'villain', 'wannab',\n",
      "       'want', 'weep', 'welcom', 'without', 'would', 'written', 'yeah',\n",
      "       'yeeeeah', 'yet'], dtype='<U25')]\n",
      "\n",
      "[   40   167   241   283   427   454   468   486   527   571   673   694\n",
      "   725   768  1115  1165  1209  1223  1256  1562  1748  1773  1866  2074\n",
      "  2359  2378  2499  2553  2763  3159  3564  3746  3809  3846  3853  3959\n",
      "  4153  4259  4290  4390  4450  4677  4706  4787  4843  4849  5302  5338\n",
      "  5560  5842  5885  6179  6189  6211  6218  6377  6403  6407  6425  6506\n",
      "  6615  6630  6691  6790  6793  6805  6904  6915  6925  6929  6998  7092\n",
      "  7121  7511  7513  7755  7800  7992  8022  8108  8166  8445  8520  8632\n",
      "  8804  8872  8992  9026  9040  9112  9440  9458  9554  9636  9701  9739\n",
      "  9753  9890  9894  9953 10057 10183 10290 10388 10426 10434 10854 11125\n",
      " 11128 11142 11242 11319 11322 11410 11437 11533 11570 11623 11636 11643\n",
      " 11722 11882 12017 12047 12276 12598 12655 12665 12691 12734 12735 12825\n",
      " 12914 13124 13125 13154 13192 13268 13276 13310 13395 13486 13492 13559\n",
      " 13735 13762 13767 13924 13931 13936 13945 13989 14022 14087 14513 14528\n",
      " 14761 15091 15115 15264 15297 15361 15405 15478 15480 15527 15627 15773\n",
      " 15914 16080 16127 16135 16144 16187 16423 16512 16743 17251 17256 17286\n",
      " 17293 17350 17434 17488 17575 17588 17594 17597 17648 17684 17685 17702\n",
      " 17746 17780 17784 17804 17823 18039 18165 18283 18368 18392 18493 18557\n",
      " 18561 18617 18655 18740 18830 18955 19089 19124 19278 19374 19504 19518\n",
      " 19596 19613 19725 19885 20021 20076 20161 20217 20291 20346 20497 20647\n",
      " 20779 20999 21038 21173 21252 21400 21483 21669 21690 21905 21906 22040\n",
      " 22062 22327 22436 22465 22550 22557 22581]\n",
      "\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 2 4 1 1 1 1 1 1 1 1 1 1 1 2 1 6 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 5 2 4 3 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 2 2 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      "  1 1 1 1 1 1 4 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 6 1 3 1 1 1 1 1 1 2 1\n",
      "  1 1 1 1 2 1 1 4 1 3 2 1 1 1 1 1 1 6 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2\n",
      "  1 1 2 1 1 3 1 1 1 2 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 2 1\n",
      "  1 1 1 1 2 1 1 4 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "row_index=0 #select one email\n",
    "print(x_train_df[row_index,:].todense().shape)\n",
    "print(\"this is the non-sparse matrix=\",x_train_df[row_index,:].todense())\n",
    "ind=np.where(x_train_df[row_index,:].todense()[0,:]>0)[1]\n",
    "print()\n",
    "#original words in the review\n",
    "print(x_train.values[row_index])\n",
    "print()\n",
    "#decoded numerical input \n",
    "print(cv.inverse_transform(x_train_df[row_index,:].todense()))\n",
    "print()\n",
    "#index of those words in x_train_df[row_index,:].todense()\n",
    "print(ind)\n",
    "print()\n",
    "# number of times those words appears in the email\n",
    "print(x_train_df[row_index,ind].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4\n",
    "#MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train_df,y_train)\n",
    "prediction_train=clf.predict(x_train_df)\n",
    "prediction_test=clf.predict(x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.96375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step5\n",
    "#scores\n",
    "print(\"Accuracy:\"+str(accuracy_score(y_train,prediction_train)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8275\n",
      "\n",
      "Confusion Matrix\n",
      "[[169  29]\n",
      " [ 40 162]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#scores\n",
    "print(\"Accuracy:\"+str(accuracy_score(y_test,prediction_test)))\n",
    "print()\n",
    "\n",
    "conf_mat=confusion_matrix(y_test, prediction_test)\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
